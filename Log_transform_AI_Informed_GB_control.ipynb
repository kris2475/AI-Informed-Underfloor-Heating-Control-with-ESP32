{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ade1f7b",
        "outputId": "2723f4e5-e4e0-4520-efc6-016aa904f568"
      },
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "SCRIPT: Log_transform_NonSkewed_LightGBM.py\n",
        "ESP32 Heater ML Pipeline - NON-SKEWED (LIGHTGBM UPGRADE)\n",
        "================================================================================\n",
        "\n",
        "PURPOSE:\n",
        "This script upgrades the algorithm to LightGBM (LGBMRegressor) while retaining the\n",
        "final, corrected feature set (thermal-only) and the non-skewed log transformation.\n",
        "The goal is to increase R-squared and lower MAE by using a more powerful algorithm\n",
        "to learn the subtle thermal dynamics.\n",
        "\n",
        "TRANSFORMATION FORMULA: y = log(1 + PWM_Target)\n",
        "ALGORITHM: LightGBM (LGBMRegressor)\n",
        "\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import lightgbm as lgb # --- NEW ALGORITHM IMPORT ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "import joblib\n",
        "\n",
        "# --- Configuration Settings (ADJUST THESE) ---\n",
        "CSV_DIRECTORY_PATH = '.'\n",
        "PREDICTION_WINDOW_STEPS = 10\n",
        "LAG_STEPS_TEMP = [1, 5, 10]\n",
        "LAG_STEPS_PWM = [5, 10]\n",
        "# ---------------------------------------------\n",
        "\n",
        "# Mapping the raw column names from the ESP32 log files\n",
        "MAP_FEATURES = {\n",
        "    'thrmstr6': 'T_Heater',\n",
        "    'thrmstrout': 'T_Outside',\n",
        "    'bme_temp': 'T_Room_BME',\n",
        "    'pwm': 'PWM_Applied',\n",
        "    'activity_sim': 'T_Target_Demand'\n",
        "}\n",
        "\n",
        "# ==========================================================\n",
        "# Step 1 & 2: Load Data and Basic Cleaning\n",
        "# ==========================================================\n",
        "csv_file_pattern = os.path.join(CSV_DIRECTORY_PATH, '*.csv')\n",
        "all_csv_files = glob.glob(csv_file_pattern)\n",
        "\n",
        "file_names = [f for f in all_csv_files\n",
        "              if os.path.basename(f).startswith('Dynamic_') or\n",
        "                 os.path.basename(f).startswith('Stabilized_')]\n",
        "\n",
        "if not file_names:\n",
        "    raise ValueError(f\"No CSV files found matching the criteria in: {CSV_DIRECTORY_PATH}\")\n",
        "\n",
        "df_list = []\n",
        "for f in file_names:\n",
        "    df = pd.read_csv(f)\n",
        "    # Tagging each row with its source file for the Run-Aware Split (Leakage Fix)\n",
        "    df['source_file'] = os.path.basename(f)\n",
        "    df_list.append(df)\n",
        "\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "combined_df.columns = [c.strip().lower() for c in combined_df.columns]\n",
        "combined_df.rename(columns=MAP_FEATURES, inplace=True)\n",
        "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
        "\n",
        "# Ensure T_Target_Demand exists (for older logs without simulated occupancy)\n",
        "if 'T_Target_Demand' not in combined_df.columns:\n",
        "    combined_df['T_Target_Demand'] = 25.0\n",
        "\n",
        "# ==========================================================\n",
        "# Step 3: Feature Engineering (Lagged & Delta Features)\n",
        "# ==========================================================\n",
        "def feature_engineer_time_series(df):\n",
        "    \"\"\"Generates the target (shifted) and lagged features.\"\"\"\n",
        "\n",
        "    # CRITICAL: Target is the PWM 10 steps (10 minutes) into the future\n",
        "    df['PWM_Target'] = df['PWM_Applied'].shift(-PREDICTION_WINDOW_STEPS)\n",
        "\n",
        "    # Generate Lagged Temperature Features (T_Heater and T_Room_BME)\n",
        "    for col in ['T_Heater', 'T_Room_BME']:\n",
        "        for lag in LAG_STEPS_TEMP:\n",
        "            df[f'{col}_lag_{lag}min'] = df[col].shift(lag)\n",
        "\n",
        "    # Generate Lagged PWM Features (THESE ARE STILL CALCULATED BUT NOT USED LATER)\n",
        "    for lag in LAG_STEPS_PWM:\n",
        "        df[f'PWM_Applied_lag_{lag}min'] = df['PWM_Applied'].shift(lag)\n",
        "\n",
        "    # Generate Differential Feature (Rate of Change)\n",
        "    df['T_Heater_Delta_5min'] = df['T_Heater'] - df['T_Heater'].shift(5)\n",
        "\n",
        "    # Drop rows where target or the longest lagged features are NaN (created by the shifts)\n",
        "    df.dropna(subset=['PWM_Target', 'T_Heater_lag_10min'], inplace=True)\n",
        "    return df\n",
        "\n",
        "combined_df_fe = feature_engineer_time_series(combined_df.copy())\n",
        "print(f\"DataFrame shape after feature engineering: {combined_df_fe.shape}\")\n",
        "\n",
        "# ==========================================================\n",
        "# Step 4: Stage B - Predict Non-Skewed Log-Transformed PWM Control\n",
        "# ==========================================================\n",
        "\n",
        "# 1. Define Final Optimized Feature List (CLEAN - ONLY THERMAL FEATURES)\n",
        "feature_cols_stage_b_optimized = [\n",
        "    'T_Heater', 'T_Outside', 'T_Room_BME', 'T_Target_Demand', 'T_Heater_Delta_5min',\n",
        "    'T_Heater_lag_1min', 'T_Heater_lag_5min', 'T_Heater_lag_10min',\n",
        "    'T_Room_BME_lag_1min', 'T_Room_BME_lag_5min', 'T_Room_BME_lag_10min',\n",
        "]\n",
        "\n",
        "X = combined_df_fe[feature_cols_stage_b_optimized]\n",
        "\n",
        "# --- ORIGINAL NON-SKEWED LOG TRANSFORMATION ---\n",
        "# Original Target: y = log(1 + PWM_Target)\n",
        "y = np.log1p(combined_df_fe['PWM_Target'])\n",
        "print(f\"Target variable created using Original Non-Skewed Log Transformation.\")\n",
        "\n",
        "\n",
        "# 2. Train/Test Split by Runs (Temporal Leakage Fix)\n",
        "unique_runs = combined_df_fe['source_file'].unique()\n",
        "train_runs, test_runs = train_test_split(unique_runs, test_size=0.265, random_state=42)\n",
        "\n",
        "train_mask_b = combined_df_fe['source_file'].isin(train_runs)\n",
        "test_mask_b = combined_df_fe['source_file'].isin(test_runs)\n",
        "\n",
        "X_train_b, X_test_b = X[train_mask_b].copy(), X[test_mask_b].copy()\n",
        "y_train_b, y_test_b = y[train_mask_b].copy(), y[test_mask_b].copy()\n",
        "\n",
        "# 3. --- CRITICAL LEAKAGE FIX: Impute missing values using only the training set mean ---\n",
        "print(\"\\nApplying Imputation Leakage Fix...\")\n",
        "imputation_cols = ['T_Heater', 'T_Outside', 'T_Room_BME']\n",
        "\n",
        "for col in imputation_cols:\n",
        "    train_mean = X_train_b[col].mean()\n",
        "    X_train_b[col] = X_train_b[col].fillna(train_mean)\n",
        "    X_test_b[col] = X_test_b[col].fillna(train_mean)\n",
        "\n",
        "print(f\"Stage B Training shape: {X_train_b.shape}, Testing shape: {X_test_b.shape}\")\n",
        "\n",
        "# 4. Train LightGBM REGRESSOR\n",
        "print(\"\\nTraining Stage B LightGBM REGRESSOR (Forced Thermal Learning)...\")\n",
        "lgbm_regressor = lgb.LGBMRegressor(\n",
        "    objective='regression_l1', # Use MAE objective\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    metric='mae'\n",
        ")\n",
        "lgbm_regressor.fit(\n",
        "    X_train_b, y_train_b,\n",
        "    eval_set=[(X_test_b, y_test_b)],\n",
        "    eval_metric='mae',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
        ")\n",
        "\n",
        "# 5. Evaluate Stage B\n",
        "y_pred_b_log = lgbm_regressor.predict(X_test_b)\n",
        "\n",
        "# --- ORIGINAL INVERSE TRANSFORMATION ---\n",
        "# Formula: PWM = e^y - 1\n",
        "y_pred_b_linear = np.expm1(y_pred_b_log)\n",
        "y_test_b_linear = np.expm1(y_test_b)\n",
        "\n",
        "\n",
        "mae_b = mean_absolute_error(y_test_b_linear, y_pred_b_linear)\n",
        "rmse_b = np.sqrt(mean_squared_error(y_test_b_linear, y_pred_b_linear))\n",
        "r2_score_linear = r2_score(y_test_b_linear, y_pred_b_linear)\n",
        "\n",
        "print(\"\\n--- Stage B Test Performance (LightGBM - Non-Skewed Log) ---\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_b:.4f}\")\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse_b:.4f}\")\n",
        "print(f\"R-squared (R2, Linear): {r2_score_linear:.4f}\")\n",
        "\n",
        "# 6. Feature Importance and Model Saving\n",
        "importances = lgbm_regressor.feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': feature_cols_stage_b_optimized, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nStage B Feature Importances (Top 5):\")\n",
        "# Note: LightGBM returns raw counts, so normalize to percentage for comparison\n",
        "total_importance = importance_df['Importance'].sum()\n",
        "importance_df['Importance (%)'] = (importance_df['Importance'] / total_importance) * 100\n",
        "print(importance_df.head(5).to_string(index=False))\n",
        "\n",
        "# Save the final model\n",
        "model_filename = 'lgbm_proactive_heater_regressor_NON_SKEWED_THERMAL.joblib'\n",
        "joblib.dump(lgbm_regressor, model_filename)\n",
        "print(f\"\\nFinal LightGBM Regressor Model saved as: {model_filename}\")\n",
        "\n",
        "print(\"\\n==========================================================\")\n",
        "print(\"Pipeline Complete: LightGBM Applied to Proactive Feature Set.\")\n",
        "print(\"==========================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame shape: (287780, 17)\n",
            "DataFrame shape after feature engineering: (287760, 28)\n",
            "Target variable created using Original Non-Skewed Log Transformation.\n",
            "\n",
            "Applying Imputation Leakage Fix...\n",
            "Stage B Training shape: (201638, 11), Testing shape: (86122, 11)\n",
            "\n",
            "Training Stage B LightGBM REGRESSOR (Forced Thermal Learning)...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2406\n",
            "[LightGBM] [Info] Number of data points in the train set: 201638, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 3.433987\n",
            "\n",
            "--- Stage B Test Performance (LightGBM - Non-Skewed Log) ---\n",
            "Mean Absolute Error (MAE): 97.9212\n",
            "Root Mean Square Error (RMSE): 201.1743\n",
            "R-squared (R2, Linear): 0.4510\n",
            "\n",
            "Stage B Feature Importances (Top 5):\n",
            "             Feature  Importance  Importance (%)\n",
            "           T_Outside        1451       31.406926\n",
            " T_Heater_Delta_5min         986       21.341991\n",
            "            T_Heater         762       16.493506\n",
            "  T_Heater_lag_10min         332        7.186147\n",
            "T_Room_BME_lag_10min         269        5.822511\n",
            "\n",
            "Final LightGBM Regressor Model saved as: lgbm_proactive_heater_regressor_NON_SKEWED_THERMAL.joblib\n",
            "\n",
            "==========================================================\n",
            "Pipeline Complete: LightGBM Applied to Proactive Feature Set.\n",
            "==========================================================\n"
          ]
        }
      ]
    }
  ]
}